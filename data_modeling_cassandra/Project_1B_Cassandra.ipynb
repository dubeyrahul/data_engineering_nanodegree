{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import etl_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from etl_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvdubey/workspace/data_eng_nanodegree/data_modeling_cassandra\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filename = 'event_datafile_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_rows_list = [] \n",
    "    \n",
    "for f in file_path_list:\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "#print(len(full_data_rows_list))\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open(data_filename, 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "with open(data_filename, 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f406bd47668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.set_keyspace('sparkify')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will create tables to answer the following queries. The query helps us determine how we should go about and create our tables, and design our schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Give me the artist, song title, and song's length in the music app event history that was during sessionId=338 and itemInSession=4 \n",
    "\n",
    "\n",
    "Note here that we need to perform a `WHERE` clause on `session` and `itemInSession`. Moreover, we see that these two columns uniquely identify a row in our events data.\n",
    "\n",
    "So we can go ahead and create `sessionized_music_table` that partitions data by `(sessionId, itemInSession)` \n",
    "\n",
    "NOTE: the `COMPOSITE PRIMARY KEY` are the first two columns in the `CREATE` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f406704b6a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "sessionized_music_table\n",
    "    (\n",
    "    sessionId int, \n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    length float,\n",
    "    PRIMARY KEY (sessionId, itemInSession))\"\"\"\n",
    "session.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's insert data into the above table using the etl_helper functions. \n",
    "\n",
    "**Note:** We need to make sure we insert values such that the primary-keys are present in same order as they are in the `COMPOSITE PRIMARY KEY` as shown in the `CREATE` statement. Thus, `sessionId` and `itemInSession` need to be the first two values we insert via our insert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into sessionized_music_table...\n",
      "Finished inserting data into sessionized_music_table\n"
     ]
    }
   ],
   "source": [
    "insert_data_in_table(\n",
    "    session, \n",
    "    data_filename, \n",
    "    'sessionized_music_table', \n",
    "    ('sessionId', 'itemInSession', 'artist', 'song', 'length')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we can run our query:**\n",
    "\n",
    "`Give me the artist, song title, and song's length in the music app event history that was during sessionId=338 and itemInSession=4`\n",
    "\n",
    "NOTE: The order of columns in the `WHERE` clause matches that of the `COMPOSITE PRIMARY KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Row(artist='Faithless', song='Music Matters (Mark Knight Dub)', length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "result = session.execute(\"\"\"\n",
    "SELECT artist, song, length FROM sessionized_music_table where sessionId=338 AND itemInSession=4\"\"\")\n",
    "\n",
    "print (f\"\"\"\\nResults:\"\"\")\n",
    "for r in result:\n",
    "    print(r)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "Note here that we need to perform a `WHERE` clause on `userId` and `sessionid`.  So we definitely need these two columns in our `COMPOSITE PRIMARY KEY`. But, we can see that this does not uniquely identify a row in our events data, since a particular userId in a particular sessionId can have multiple itemInSession. \n",
    "\n",
    "So, we need to add `itemInSession` to our `COMPOSITE PRIMARY KEY` to uniquely identify a row. Moreover, we need to return results such that they are sorted by `itemInSession` so we need to make this our `CLUSTERING COLUMN`. We can choose to sort this be ASC or DESC order.\n",
    "\n",
    "So we can go ahead and create `user_sessionized_music_table` that partitions data by `(userId, sessionId)` and sorts them by `itemInSession`.  \n",
    "Thus, our `PRIMARY KEY` becomes `((userId, sessionId), itemInSession)`\n",
    "\n",
    "NOTE: the `COMPOSITE PRIMARY KEY` are the first three columns in the `CREATE` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f40670be940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "user_sessionized_music_table\n",
    "    (\n",
    "    userId int,\n",
    "    sessionId int,\n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    firstName text,\n",
    "    lastName text,\n",
    "    PRIMARY KEY ((userId, sessionId), itemInSession)\n",
    "    )\n",
    "    WITH CLUSTERING ORDER BY (itemInSession DESC);\"\"\"\n",
    "session.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's insert data into the above table using the etl_helper functions. \n",
    "\n",
    "**Note:** We need to make sure we insert values such that the primary-keys are present in same order as they are in the `COMPOSITE PRIMARY KEY` as shown in the `CREATE` statement. Thus, `userId`, `sessionId`, and `itemInSession` need to be the first three values **(in the same order)** we insert via our insert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into user_sessionized_music_table...\n",
      "Finished inserting data into user_sessionized_music_table\n"
     ]
    }
   ],
   "source": [
    "insert_data_in_table(\n",
    "    session, \n",
    "    data_filename, \n",
    "    'user_sessionized_music_table', \n",
    "    ('userId', 'sessionId', 'itemInSession', 'artist', 'song', 'firstName', 'lastName')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we can run our query:**\n",
    "\n",
    "`Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182`\n",
    "\n",
    "NOTE: The order of columns in the `WHERE` clause matches that of the `COMPOSITE PRIMARY KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', firstname='Sylvie', lastname='Cruz', iteminsession=3)\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', firstname='Sylvie', lastname='Cruz', iteminsession=2)\n",
      "Row(artist='Three Drives', song='Greece 2000', firstname='Sylvie', lastname='Cruz', iteminsession=1)\n",
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", firstname='Sylvie', lastname='Cruz', iteminsession=0)\n"
     ]
    }
   ],
   "source": [
    "print (f\"\"\"\\nResults:\"\"\")\n",
    "result = session.execute(\"\"\"\n",
    "SELECT artist, song, firstName, lastName, itemInSession FROM user_sessionized_music_table where userId=10 AND sessionId=182\"\"\")\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "Note here that we need to perform a `WHERE` clause on `song`.  So we definitely need this columns in our `PRIMARY KEY`. But, we know that we cannot uniquely identify a row just by the `song` so we can pair it with `userId` (*since this query **only** cares about the users who listen to a particular song, and not about the frequency with which they listen to a song, if we cared about frequency then we would have to include **sessionId** and **itemInSession** too*)\n",
    "\n",
    "So, we need to add `song` and `userId` in our `COMPOSITE PRIMARY KEY` to uniquely identify a row. But we need to make sure that `song` is the first column in our `COMPOSITE PRIMARY KEY` since we are querying by it. \n",
    "\n",
    "So we can go ahead and create `user_songs_table` that partitions data by `(song, userId)`\n",
    "\n",
    "NOTE: the `COMPOSITE PRIMARY KEY` are the first two columns in the `CREATE` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f40670b44a8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "user_songs_table\n",
    "    (\n",
    "    song text,\n",
    "    userId int,\n",
    "    firstName text,\n",
    "    lastName text,\n",
    "    PRIMARY KEY (song, userId)\n",
    "    );\"\"\"\n",
    "session.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert data into the above table using the etl_helper functions. \n",
    "\n",
    "**Note:** We need to make sure we insert values such that the primary-keys are present in same order as they are in the `COMPOSITE PRIMARY KEY` as shown in the `CREATE` statement. Thus, `song` and `userId` need to be the first two values **(in the same order)** we insert via our insert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into user_songs_table...\n",
      "Finished inserting data into user_songs_table\n"
     ]
    }
   ],
   "source": [
    "insert_data_in_table(\n",
    "    session,\n",
    "    data_filename, \n",
    "    'user_songs_table', \n",
    "    ('song', 'userId','firstName', 'lastName')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can run our query: ** \n",
    "\n",
    "`Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'`\n",
    "\n",
    "NOTE: The order of columns in the `WHERE` clause matches that of the `COMPOSITE PRIMARY KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Row(firstname='Jacqueline', lastname='Lynch')\n",
      "Row(firstname='Tegan', lastname='Levine')\n",
      "Row(firstname='Sara', lastname='Johnson')\n"
     ]
    }
   ],
   "source": [
    "print (f\"\"\"\\nResults:\"\"\")\n",
    "result = session.execute(\"\"\"\n",
    "SELECT  firstName, lastName  FROM user_songs_table where song='All Hands Against His Own'\"\"\")\n",
    "for r in result:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessionized_music_table\n",
      "user_sessionized_music_table\n",
      "user_songs_table\n"
     ]
    }
   ],
   "source": [
    "table_list = session.execute(\"\"\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'sparkify';\"\"\")\n",
    "for t in table_list:\n",
    "    print(t.table_name)\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {t.table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "389px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
